{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate YOLOv8 Detection Models\n",
    "**Author:** G8  \n",
    "**Task:** 3.2 - YOLOv8 Evaluation  \n",
    "**Timeline:** Feb 10, 2025  \n",
    "\n",
    "**Purpose:**\n",
    "- Evaluate all 3 YOLOv8 models\n",
    "- Calculate mAP50, mAP50-95, precision, recall\n",
    "- Measure inference speed (FPS)\n",
    "- Compare model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "\n",
    "print(\"Libraries imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "PROJECT_ROOT = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "DATA_YAML = PROJECT_ROOT / \"data\" / \"data.yaml\"\n",
    "MODELS_PATH = PROJECT_ROOT / \"models\" / \"detection\"\n",
    "RESULTS_PATH = PROJECT_ROOT / \"results\" / \"detection\"\n",
    "\n",
    "(RESULTS_PATH / 'metrics').mkdir(parents=True, exist_ok=True)\n",
    "(RESULTS_PATH / 'visualizations').mkdir(parents=True, exist_ok=True)\n",
    "(RESULTS_PATH / 'predictions').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Results path: {RESULTS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_yolo_model(variant):\n",
    "    \"\"\"Evaluate one YOLOv8 model\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"EVALUATING YOLOV8{variant.upper()}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load model\n",
    "    model_path = MODELS_PATH / f\"yolov8{variant}_best.pt\"\n",
    "    print(f\"\\nLoading: {model_path.name}\")\n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    # Run validation\n",
    "    print(\"\\nRunning validation...\")\n",
    "    metrics = model.val(\n",
    "        data=str(DATA_YAML),\n",
    "        split='test',\n",
    "        save_json=True,\n",
    "        conf=0.25,\n",
    "        iou=0.6,\n",
    "        plots=True\n",
    "    )\n",
    "    \n",
    "    # Extract metrics\n",
    "    results = {\n",
    "        'model': f'yolov8{variant}',\n",
    "        'mAP50': float(metrics.box.map50),\n",
    "        'mAP50_95': float(metrics.box.map),\n",
    "        'precision': float(metrics.box.mp),\n",
    "        'recall': float(metrics.box.mr),\n",
    "    }\n",
    "    \n",
    "    # Measure speed\n",
    "    print(\"\\nMeasuring inference speed...\")\n",
    "    test_imgs = list((PROJECT_ROOT / \"data\" / \"multi_objects\" / \"images\" / \"test\").glob(\"*.jpg\"))\n",
    "    \n",
    "    if len(test_imgs) > 0:\n",
    "        # Warmup\n",
    "        for _ in range(5):\n",
    "            _ = model(test_imgs[0], verbose=False)\n",
    "        \n",
    "        # Measure\n",
    "        start = time.time()\n",
    "        num_runs = 50\n",
    "        for i in range(num_runs):\n",
    "            _ = model(test_imgs[i % len(test_imgs)], verbose=False)\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        results['inference_fps'] = float(num_runs / elapsed)\n",
    "        results['inference_ms'] = float((elapsed / num_runs) * 1000)\n",
    "    \n",
    "    # Model size\n",
    "    results['model_size_mb'] = float(os.path.getsize(model_path) / (1024 * 1024))\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"METRICS SUMMARY\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"mAP50:      {results['mAP50']:.4f}\")\n",
    "    print(f\"mAP50-95:   {results['mAP50_95']:.4f}\")\n",
    "    print(f\"Precision:  {results['precision']:.4f}\")\n",
    "    print(f\"Recall:     {results['recall']:.4f}\")\n",
    "    print(f\"FPS:        {results['inference_fps']:.1f}\")\n",
    "    print(f\"Latency:    {results['inference_ms']:.2f} ms\")\n",
    "    print(f\"Model size: {results['model_size_mb']:.2f} MB\")\n",
    "    \n",
    "    # Save metrics\n",
    "    json_path = RESULTS_PATH / 'metrics' / f\"yolov8{variant}_metrics.json\"\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"\\nMetrics saved: {json_path.name}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate all models\n",
    "print(\"=\"*80)\n",
    "print(\"EVALUATION PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_metrics = []\n",
    "for variant in ['n', 's', 'm']:\n",
    "    metrics = evaluate_yolo_model(variant)\n",
    "    all_metrics.append(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_comp = pd.DataFrame(all_metrics)\n",
    "df_comp = df_comp.sort_values('mAP50', ascending=False)\n",
    "df_comp['rank'] = range(1, len(df_comp) + 1)\n",
    "\n",
    "# Reorder\n",
    "cols = ['rank', 'model', 'mAP50', 'mAP50_95', 'precision', 'recall',\n",
    "        'inference_fps', 'inference_ms', 'model_size_mb']\n",
    "df_comp = df_comp[cols]\n",
    "\n",
    "print(\"\\n\" + df_comp.to_string(index=False))\n",
    "\n",
    "# Save Excel\n",
    "excel_path = RESULTS_PATH / 'metrics' / 'yolo_comparison.xlsx'\n",
    "df_comp.to_excel(excel_path, index=False, sheet_name='YOLOv8_Comparison')\n",
    "print(f\"\\nSaved: {excel_path.name}\")\n",
    "\n",
    "# Best model\n",
    "best = df_comp.iloc[0]\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEST DETECTION MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model: {best['model']}\")\n",
    "print(f\"mAP50: {best['mAP50']:.4f}\")\n",
    "print(f\"FPS: {best['inference_fps']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison charts\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# mAP50 comparison\n",
    "ax1 = axes[0]\n",
    "bars1 = ax1.barh(df_comp['model'], df_comp['mAP50'])\n",
    "ax1.set_xlabel('mAP50', fontsize=12)\n",
    "ax1.set_title('Detection Accuracy (mAP50)', fontsize=14, fontweight='bold')\n",
    "ax1.axvline(x=0.85, color='r', linestyle='--', label='Target (0.85)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "for i, bar in enumerate(bars1):\n",
    "    width = bar.get_width()\n",
    "    ax1.text(width, bar.get_y() + bar.get_height()/2,\n",
    "             f'{width:.3f}', ha='left', va='center')\n",
    "\n",
    "# FPS comparison\n",
    "ax2 = axes[1]\n",
    "bars2 = ax2.barh(df_comp['model'], df_comp['inference_fps'])\n",
    "ax2.set_xlabel('FPS (Frames Per Second)', fontsize=12)\n",
    "ax2.set_title('Inference Speed', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "for i, bar in enumerate(bars2):\n",
    "    width = bar.get_width()\n",
    "    ax2.text(width, bar.get_y() + bar.get_height()/2,\n",
    "             f'{width:.1f}', ha='left', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_PATH / 'metrics' / 'yolo_comparison.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"Comparison chart saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.2 Complete!\n",
    "\n",
    "**Next:** Task 4 - Build Streamlit application (Kevin)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
