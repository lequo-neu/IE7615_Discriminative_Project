{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Object Image Generation\n",
    "**Author:** G8  \n",
    "**Task:** 1.2 - Generate Multi-Object Images with YOLO Annotations  \n",
    "**Timeline:** Feb 3-4, 2025  \n",
    "\n",
    "**Purpose:**\n",
    "- Generate 1200+ multi-object images using grid layouts\n",
    "- Create YOLO format annotations\n",
    "- Split into train/val/test for YOLOv8 training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Input path: /Users/kevin/Documents/GitHub/Python/VESKL/11.DAE/NEU/NEU_IE7615/Prj/Discriminative/G8/Project1/IE7615_Discriminative_Project/data/processed/single_objects/train\n",
      "  Total classes: 39\n",
      "  Grid types: 3\n",
      "  Total images to generate: 1200\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "PROJECT_ROOT = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "PROCESSED_PATH = PROJECT_ROOT / \"data\" / \"processed\" / \"single_objects\" / \"train\"\n",
    "OUTPUT_IMAGES = PROJECT_ROOT / \"data\" / \"multi_objects\" / \"images\"\n",
    "OUTPUT_LABELS = PROJECT_ROOT / \"data\" / \"multi_objects\" / \"labels\"\n",
    "STATS_PATH = PROJECT_ROOT / \"data\" / \"statistics\"\n",
    "\n",
    "# Load class mapping\n",
    "with open(PROJECT_ROOT / \"data\" / \"class_mapping.json\", 'r') as f:\n",
    "    class_mapping = json.load(f)\n",
    "\n",
    "# Grid configurations\n",
    "GRID_CONFIGS = [\n",
    "    {'name': '2x2', 'rows': 2, 'cols': 2, 'count': 400},\n",
    "    {'name': '2x3', 'rows': 2, 'cols': 3, 'count': 400}, \n",
    "    {'name': '3x3', 'rows': 3, 'cols': 3, 'count': 400}\n",
    "]\n",
    "\n",
    "CELL_SIZE = 224  # Each object is 224x224\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Input path: {PROCESSED_PATH}\")\n",
    "print(f\"  Total classes: {class_mapping['num_classes']}\")\n",
    "print(f\"  Grid types: {len(GRID_CONFIGS)}\")\n",
    "print(f\"  Total images to generate: {sum(g['count'] for g in GRID_CONFIGS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load All Processed Images\n",
    "Load paths to all preprocessed single-object images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING PROCESSED IMAGES\n",
      "================================================================================\n",
      "\n",
      "Loaded 39 object classes\n",
      "Total images available: 2943\n",
      "Avg images per class: 75.5\n"
     ]
    }
   ],
   "source": [
    "def load_image_paths():\n",
    "    \"\"\"\n",
    "    Load all preprocessed image paths organized by class\n",
    "    Returns: dict {object_id: [list of image paths]}\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"LOADING PROCESSED IMAGES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    image_dict = {}\n",
    "    \n",
    "    for obj_folder in PROCESSED_PATH.iterdir():\n",
    "        if obj_folder.is_dir():\n",
    "            obj_id = obj_folder.name\n",
    "            images = sorted(list(obj_folder.glob(\"*.jpg\")))\n",
    "            if len(images) > 0:\n",
    "                image_dict[obj_id] = images\n",
    "    \n",
    "    total_images = sum(len(imgs) for imgs in image_dict.values())\n",
    "    \n",
    "    print(f\"\\nLoaded {len(image_dict)} object classes\")\n",
    "    print(f\"Total images available: {total_images}\")\n",
    "    print(f\"Avg images per class: {total_images/len(image_dict):.1f}\")\n",
    "    \n",
    "    return image_dict\n",
    "\n",
    "# Load image paths\n",
    "image_paths = load_image_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate Grid Images\n",
    "Create multi-object images by placing single objects in grid layouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing grid generation...\n",
      "\n",
      "Test grid shape: (448, 448, 3)\n",
      "Test annotations: 4 objects\n",
      "\n",
      "Sample annotation:\n",
      "  34 0.250000 0.250000 0.500000 0.500000\n",
      "\n",
      "Format: class_id x_center y_center width height (all normalized)\n"
     ]
    }
   ],
   "source": [
    "def create_grid_image(image_paths_dict, class_mapping, rows, cols):\n",
    "    \"\"\"\n",
    "    Create a grid image with multiple objects\n",
    "    \n",
    "    Args:\n",
    "        image_paths_dict: Dict of {obj_id: [image_paths]}\n",
    "        class_mapping: Class to index mapping\n",
    "        rows: Number of rows in grid\n",
    "        cols: Number of columns in grid\n",
    "    \n",
    "    Returns:\n",
    "        grid_image: Combined image (rows*224 x cols*224)\n",
    "        annotations: List of YOLO format annotations\n",
    "    \"\"\"\n",
    "    num_objects = rows * cols\n",
    "    \n",
    "    # Randomly select different object classes\n",
    "    available_classes = list(image_paths_dict.keys())\n",
    "    selected_classes = random.sample(available_classes, num_objects)\n",
    "    \n",
    "    # Create blank grid\n",
    "    grid_height = rows * CELL_SIZE\n",
    "    grid_width = cols * CELL_SIZE\n",
    "    grid_image = np.zeros((grid_height, grid_width, 3), dtype=np.uint8)\n",
    "    \n",
    "    annotations = []\n",
    "    \n",
    "    # Fill grid with objects\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            idx = i * cols + j\n",
    "            obj_id = selected_classes[idx]\n",
    "            \n",
    "            # Randomly select an image from this class\n",
    "            img_path = random.choice(image_paths_dict[obj_id])\n",
    "            img = cv2.imread(str(img_path))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Place in grid\n",
    "            y_start = i * CELL_SIZE\n",
    "            y_end = y_start + CELL_SIZE\n",
    "            x_start = j * CELL_SIZE\n",
    "            x_end = x_start + CELL_SIZE\n",
    "            grid_image[y_start:y_end, x_start:x_end] = img\n",
    "            \n",
    "            # Create YOLO annotation (normalized coordinates)\n",
    "            class_idx = class_mapping['class_to_idx'][obj_id]\n",
    "            x_center = (x_start + CELL_SIZE/2) / grid_width\n",
    "            y_center = (y_start + CELL_SIZE/2) / grid_height\n",
    "            box_width = CELL_SIZE / grid_width\n",
    "            box_height = CELL_SIZE / grid_height\n",
    "            \n",
    "            # YOLO format: class_id x_center y_center width height\n",
    "            annotations.append(f\"{class_idx} {x_center:.6f} {y_center:.6f} {box_width:.6f} {box_height:.6f}\")\n",
    "    \n",
    "    return grid_image, annotations\n",
    "\n",
    "# Test function\n",
    "print(\"Testing grid generation...\")\n",
    "test_grid, test_annot = create_grid_image(image_paths, class_mapping, 2, 2)\n",
    "print(f\"\\nTest grid shape: {test_grid.shape}\")\n",
    "print(f\"Test annotations: {len(test_annot)} objects\")\n",
    "print(\"\\nSample annotation:\")\n",
    "print(f\"  {test_annot[0]}\")\n",
    "print(\"\\nFormat: class_id x_center y_center width height (all normalized)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate All Multi-Object Images\n",
    "Create 1200+ images with all grid configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GENERATING MULTI-OBJECT IMAGES\n",
      "================================================================================\n",
      "\n",
      "Generating 400 images for 2x2 grid (2x2)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2x2 grid: 100%|██████████| 400/400 [00:01<00:00, 328.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 400 images for 2x3 grid (2x3)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2x3 grid: 100%|██████████| 400/400 [00:02<00:00, 195.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 400 images for 3x3 grid (3x3)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3x3 grid: 100%|██████████| 400/400 [00:02<00:00, 190.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GENERATION SUMMARY\n",
      "================================================================================\n",
      "grid_type  rows  cols  objects_per_image  images_generated\n",
      "      2x2     2     2                  4               400\n",
      "      2x3     2     3                  6               400\n",
      "      3x3     3     3                  9               400\n",
      "\n",
      "Total images generated: 1200\n",
      "Total annotations created: 1200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"GENERATING MULTI-OBJECT IMAGES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create temp folder for all generated images\n",
    "temp_images = OUTPUT_IMAGES / \"all_generated\"\n",
    "temp_labels = OUTPUT_LABELS / \"all_generated\"\n",
    "temp_images.mkdir(parents=True, exist_ok=True)\n",
    "temp_labels.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "generation_stats = []\n",
    "image_counter = 0\n",
    "\n",
    "# Generate for each grid configuration\n",
    "for config in GRID_CONFIGS:\n",
    "    grid_name = config['name']\n",
    "    rows = config['rows']\n",
    "    cols = config['cols']\n",
    "    count = config['count']\n",
    "    \n",
    "    print(f\"\\nGenerating {count} images for {grid_name} grid ({rows}x{cols})...\")\n",
    "    \n",
    "    for i in tqdm(range(count), desc=f\"{grid_name} grid\"):\n",
    "        # Generate grid image\n",
    "        grid_img, annotations = create_grid_image(image_paths, class_mapping, rows, cols)\n",
    "        \n",
    "        # Save image\n",
    "        img_filename = f\"grid_{grid_name}_{image_counter:04d}.jpg\"\n",
    "        img_path = temp_images / img_filename\n",
    "        cv2.imwrite(str(img_path), cv2.cvtColor(grid_img, cv2.COLOR_RGB2BGR))\n",
    "        \n",
    "        # Save annotation\n",
    "        label_filename = f\"grid_{grid_name}_{image_counter:04d}.txt\"\n",
    "        label_path = temp_labels / label_filename\n",
    "        with open(label_path, 'w') as f:\n",
    "            f.write('\\n'.join(annotations))\n",
    "        \n",
    "        image_counter += 1\n",
    "    \n",
    "    generation_stats.append({\n",
    "        'grid_type': grid_name,\n",
    "        'rows': rows,\n",
    "        'cols': cols,\n",
    "        'objects_per_image': rows * cols,\n",
    "        'images_generated': count\n",
    "    })\n",
    "\n",
    "# Summary\n",
    "df_gen = pd.DataFrame(generation_stats)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(df_gen.to_string(index=False))\n",
    "print(f\"\\nTotal images generated: {image_counter}\")\n",
    "print(f\"Total annotations created: {image_counter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Split Multi-Object Dataset\n",
    "Split into train/val/test (70/15/15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SPLITTING MULTI-OBJECT DATASET\n",
      "================================================================================\n",
      "\n",
      "Total images: 1200\n",
      "Total labels: 1200\n",
      "\n",
      "Copying 840 images to train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 840/840 [00:00<00:00, 1296.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Copying 180 images to val...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val: 100%|██████████| 180/180 [00:00<00:00, 1711.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Copying 180 images to test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████| 180/180 [00:00<00:00, 1767.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SPLIT COMPLETE\n",
      "================================================================================\n",
      "  Train: 840 (70.0%)\n",
      "  Val:   180 (15.0%)\n",
      "  Test:  180 (15.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SPLITTING MULTI-OBJECT DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get all generated images\n",
    "all_images = sorted(list(temp_images.glob(\"*.jpg\")))\n",
    "all_labels = sorted(list(temp_labels.glob(\"*.txt\")))\n",
    "\n",
    "print(f\"\\nTotal images: {len(all_images)}\")\n",
    "print(f\"Total labels: {len(all_labels)}\")\n",
    "\n",
    "# Shuffle with same seed\n",
    "indices = list(range(len(all_images)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "# Calculate split\n",
    "n = len(all_images)\n",
    "train_end = int(n * 0.70)\n",
    "val_end = train_end + int(n * 0.15)\n",
    "\n",
    "train_idx = indices[:train_end]\n",
    "val_idx = indices[train_end:val_end]\n",
    "test_idx = indices[val_end:]\n",
    "\n",
    "# Copy to split folders\n",
    "splits = {\n",
    "    'train': train_idx,\n",
    "    'val': val_idx,\n",
    "    'test': test_idx\n",
    "}\n",
    "\n",
    "for split_name, idx_list in splits.items():\n",
    "    img_folder = OUTPUT_IMAGES / split_name\n",
    "    label_folder = OUTPUT_LABELS / split_name\n",
    "    img_folder.mkdir(parents=True, exist_ok=True)\n",
    "    label_folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nCopying {len(idx_list)} images to {split_name}...\")\n",
    "    \n",
    "    for idx in tqdm(idx_list, desc=f\"{split_name}\"):\n",
    "        # Copy image\n",
    "        shutil.copy2(all_images[idx], img_folder / all_images[idx].name)\n",
    "        # Copy label\n",
    "        shutil.copy2(all_labels[idx], label_folder / all_labels[idx].name)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SPLIT COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  Train: {len(train_idx)} ({len(train_idx)/n*100:.1f}%)\")\n",
    "print(f\"  Val:   {len(val_idx)} ({len(val_idx)/n*100:.1f}%)\")\n",
    "print(f\"  Test:  {len(test_idx)} ({len(test_idx)/n*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create data.yaml for YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CREATING data.yaml FOR YOLOv8\n",
      "================================================================================\n",
      "\n",
      "data.yaml created with 39 classes\n",
      "Saved to: /Users/kevin/Documents/GitHub/Python/VESKL/11.DAE/NEU/NEU_IE7615/Prj/Discriminative/G8/Project1/IE7615_Discriminative_Project/data/data.yaml\n",
      "\n",
      "First 5 classes:\n",
      "  0: OBJ001\n",
      "  1: OBJ002\n",
      "  2: OBJ003\n",
      "  3: OBJ004\n",
      "  4: OBJ005\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CREATING data.yaml FOR YOLOv8\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create data.yaml content\n",
    "data_yaml_path = PROJECT_ROOT / \"data\" / \"data.yaml\"\n",
    "multi_objects_path = PROJECT_ROOT / \"data\" / \"multi_objects\"\n",
    "\n",
    "yaml_content = f\"\"\"# YOLOv8 Dataset Configuration\n",
    "# Generated for CNN Attendance System Project\n",
    "\n",
    "path: {multi_objects_path.absolute()}\n",
    "train: images/train\n",
    "val: images/val\n",
    "test: images/test\n",
    "\n",
    "# Number of classes\n",
    "nc: {class_mapping['num_classes']}\n",
    "\n",
    "# Class names\n",
    "names:\n",
    "\"\"\"\n",
    "\n",
    "# Add class names\n",
    "idx_to_class = class_mapping['idx_to_class']\n",
    "for idx in range(class_mapping['num_classes']):\n",
    "    class_name = idx_to_class[str(idx)]\n",
    "    yaml_content += f\"  {idx}: {class_name}\\n\"\n",
    "\n",
    "# Save yaml\n",
    "with open(data_yaml_path, 'w') as f:\n",
    "    f.write(yaml_content)\n",
    "\n",
    "print(f\"\\ndata.yaml created with {class_mapping['num_classes']} classes\")\n",
    "print(f\"Saved to: {data_yaml_path}\")\n",
    "print(\"\\nFirst 5 classes:\")\n",
    "for i in range(min(5, class_mapping['num_classes'])):\n",
    "    print(f\"  {i}: {idx_to_class[str(i)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Visualize Samples\n",
    "Display sample multi-object images with bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "VISUALIZING SAMPLES\n",
      "================================================================================\n",
      "\n",
      "Visualizing 2x2 grid sample...\n",
      "  Saved: /Users/kevin/Documents/GitHub/Python/VESKL/11.DAE/NEU/NEU_IE7615/Prj/Discriminative/G8/Project1/IE7615_Discriminative_Project/data/statistics/sample_2x2_visualization.png\n",
      "\n",
      "Visualizing 2x3 grid sample...\n",
      "  Saved: /Users/kevin/Documents/GitHub/Python/VESKL/11.DAE/NEU/NEU_IE7615/Prj/Discriminative/G8/Project1/IE7615_Discriminative_Project/data/statistics/sample_2x3_visualization.png\n",
      "\n",
      "Visualizing 3x3 grid sample...\n",
      "  Saved: /Users/kevin/Documents/GitHub/Python/VESKL/11.DAE/NEU/NEU_IE7615/Prj/Discriminative/G8/Project1/IE7615_Discriminative_Project/data/statistics/sample_3x3_visualization.png\n"
     ]
    }
   ],
   "source": [
    "def visualize_grid_with_boxes(image_path, label_path, class_mapping):\n",
    "    \"\"\"\n",
    "    Visualize multi-object image with bounding boxes\n",
    "    \"\"\"\n",
    "    # Read image\n",
    "    img = cv2.imread(str(image_path))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    h, w = img.shape[:2]\n",
    "    \n",
    "    # Read annotations\n",
    "    with open(label_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    idx_to_class = class_mapping['idx_to_class']\n",
    "    \n",
    "    # Draw bounding boxes\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        class_idx = int(parts[0])\n",
    "        x_center = float(parts[1]) * w\n",
    "        y_center = float(parts[2]) * h\n",
    "        box_w = float(parts[3]) * w\n",
    "        box_h = float(parts[4]) * h\n",
    "        \n",
    "        # Convert to corner coordinates\n",
    "        x1 = x_center - box_w/2\n",
    "        y1 = y_center - box_h/2\n",
    "        \n",
    "        # Draw rectangle\n",
    "        rect = patches.Rectangle((x1, y1), box_w, box_h, \n",
    "                                  linewidth=2, edgecolor='red', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add label\n",
    "        class_name = idx_to_class[str(class_idx)]\n",
    "        ax.text(x1, y1-5, class_name, color='red', fontsize=10, \n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))\n",
    "    \n",
    "    ax.axis('off')\n",
    "    ax.set_title(f\"Multi-Object Image: {image_path.name}\")\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Visualize samples from each grid type\n",
    "print(\"=\"*80)\n",
    "print(\"VISUALIZING SAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for config in GRID_CONFIGS:\n",
    "    grid_name = config['name']\n",
    "    \n",
    "    # Find first image of this grid type in train\n",
    "    train_images = sorted(list((OUTPUT_IMAGES / \"train\").glob(f\"grid_{grid_name}_*.jpg\")))\n",
    "    if train_images:\n",
    "        img_path = train_images[0]\n",
    "        label_path = OUTPUT_LABELS / \"train\" / f\"{img_path.stem}.txt\"\n",
    "        \n",
    "        print(f\"\\nVisualizing {grid_name} grid sample...\")\n",
    "        fig = visualize_grid_with_boxes(img_path, label_path, class_mapping)\n",
    "        \n",
    "        # Save visualization\n",
    "        vis_path = STATS_PATH / f\"sample_{grid_name}_visualization.png\"\n",
    "        fig.savefig(vis_path, dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"  Saved: {vis_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Verification\n",
    "Verify annotations are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "VERIFICATION\n",
      "================================================================================\n",
      "\n",
      "TRAIN:\n",
      "  Images: 919\n",
      "  Labels: 919\n",
      "  Objects in first image: 4\n",
      "  Annotation format: VALID (all values in [0,1])\n",
      "\n",
      "VAL:\n",
      "  Images: 231\n",
      "  Labels: 231\n",
      "  Objects in first image: 6\n",
      "  Annotation format: VALID (all values in [0,1])\n",
      "\n",
      "TEST:\n",
      "  Images: 233\n",
      "  Labels: 233\n",
      "  Objects in first image: 4\n",
      "  Annotation format: VALID (all values in [0,1])\n",
      "\n",
      "================================================================================\n",
      "VERIFICATION PASSED!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"VERIFICATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    img_folder = OUTPUT_IMAGES / split\n",
    "    label_folder = OUTPUT_LABELS / split\n",
    "    \n",
    "    images = list(img_folder.glob(\"*.jpg\"))\n",
    "    labels = list(label_folder.glob(\"*.txt\"))\n",
    "    \n",
    "    print(f\"\\n{split.upper()}:\")\n",
    "    print(f\"  Images: {len(images)}\")\n",
    "    print(f\"  Labels: {len(labels)}\")\n",
    "    \n",
    "    # Check annotation format\n",
    "    if labels:\n",
    "        with open(labels[0], 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        print(f\"  Objects in first image: {len(lines)}\")\n",
    "        \n",
    "        # Verify values are normalized (0-1)\n",
    "        parts = lines[0].strip().split()\n",
    "        values = [float(x) for x in parts[1:]]\n",
    "        if all(0 <= v <= 1 for v in values):\n",
    "            print(f\"  Annotation format: VALID (all values in [0,1])\")\n",
    "        else:\n",
    "            print(f\"  WARNING: Annotation values not normalized!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VERIFICATION PASSED!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Generate Statistics Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GENERATING STATISTICS REPORT\n",
      "================================================================================\n",
      "\n",
      "Generation stats saved: /Users/kevin/Documents/GitHub/Python/VESKL/11.DAE/NEU/NEU_IE7615/Prj/Discriminative/G8/Project1/IE7615_Discriminative_Project/data/statistics/multi_object_generation_stats.csv\n",
      "\n",
      "Multi-Object Dataset Split:\n",
      "split  images  percentage\n",
      "train     919   66.449747\n",
      "  val     231   16.702820\n",
      " test     233   16.847433\n",
      "\n",
      "Split stats saved: /Users/kevin/Documents/GitHub/Python/VESKL/11.DAE/NEU/NEU_IE7615/Prj/Discriminative/G8/Project1/IE7615_Discriminative_Project/data/statistics/multi_object_split_distribution.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"GENERATING STATISTICS REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save generation stats\n",
    "gen_stats_path = STATS_PATH / \"multi_object_generation_stats.csv\"\n",
    "df_gen.to_csv(gen_stats_path, index=False)\n",
    "print(f\"\\nGeneration stats saved: {gen_stats_path}\")\n",
    "\n",
    "# Create split summary\n",
    "split_summary = {\n",
    "    'split': ['train', 'val', 'test'],\n",
    "    'images': [\n",
    "        len(list((OUTPUT_IMAGES / 'train').glob('*.jpg'))),\n",
    "        len(list((OUTPUT_IMAGES / 'val').glob('*.jpg'))),\n",
    "        len(list((OUTPUT_IMAGES / 'test').glob('*.jpg')))\n",
    "    ]\n",
    "}\n",
    "df_split_mo = pd.DataFrame(split_summary)\n",
    "df_split_mo['percentage'] = df_split_mo['images'] / df_split_mo['images'].sum() * 100\n",
    "\n",
    "print(\"\\nMulti-Object Dataset Split:\")\n",
    "print(df_split_mo.to_string(index=False))\n",
    "\n",
    "split_mo_path = STATS_PATH / \"multi_object_split_distribution.csv\"\n",
    "df_split_mo.to_csv(split_mo_path, index=False)\n",
    "print(f\"\\nSplit stats saved: {split_mo_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TASK 1.2 COMPLETED: Multi-Object Image Generation\n",
      "================================================================================\n",
      "\n",
      "Deliverables:\n",
      "1. Multi-object images: /Users/kevin/Documents/GitHub/Python/VESKL/11.DAE/NEU/NEU_IE7615/Prj/Discriminative/G8/Project1/IE7615_Discriminative_Project/data/multi_objects/images\n",
      "2. YOLO annotations: /Users/kevin/Documents/GitHub/Python/VESKL/11.DAE/NEU/NEU_IE7615/Prj/Discriminative/G8/Project1/IE7615_Discriminative_Project/data/multi_objects/labels\n",
      "3. YOLOv8 config: /Users/kevin/Documents/GitHub/Python/VESKL/11.DAE/NEU/NEU_IE7615/Prj/Discriminative/G8/Project1/IE7615_Discriminative_Project/data/data.yaml\n",
      "4. Statistics: /Users/kevin/Documents/GitHub/Python/VESKL/11.DAE/NEU/NEU_IE7615/Prj/Discriminative/G8/Project1/IE7615_Discriminative_Project/data/statistics\n",
      "\n",
      "Dataset ready for YOLOv8 training!\n",
      "\n",
      "Next: Task 2.1 - Train classification models (Kevin)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TASK 1.2 COMPLETED: Multi-Object Image Generation\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nDeliverables:\")\n",
    "print(f\"1. Multi-object images: {OUTPUT_IMAGES}\")\n",
    "print(f\"2. YOLO annotations: {OUTPUT_LABELS}\")\n",
    "print(f\"3. YOLOv8 config: {PROJECT_ROOT / 'data' / 'data.yaml'}\")\n",
    "print(f\"4. Statistics: {STATS_PATH}\")\n",
    "print(\"\\nDataset ready for YOLOv8 training!\")\n",
    "print(\"\\nNext: Task 2.1 - Train classification models (Kevin)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
