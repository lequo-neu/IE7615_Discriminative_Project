{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train ALL 4 Classification Models (Optimized)\n",
    "**Author:** G8  \n",
    "**Task:** 2.1 - Train 4 CNN Models  \n",
    "**Strategy:** Unfreeze top layers for ALL transfer learning models  \n",
    "\n",
    "**Models:**\n",
    "1. Custom CNN (simplified architecture)\n",
    "2. ResNet50 (with unfrozen top 30 layers)\n",
    "3. EfficientNet-B0 (with unfrozen top 20 layers)\n",
    "4. MobileNetV2 (with unfrozen top 20 layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete\n",
      "Libraries imported!\n",
      "TensorFlow: 2.16.2\n",
      "GPU: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import ResNet50, EfficientNetB0, MobileNetV2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"Setup complete\")\n",
    "\n",
    "print(\"Libraries imported!\")\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"GPU: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: 9\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "PROJECT_ROOT = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "DATA_PATH = PROJECT_ROOT / \"data\" / \"processed\" / \"single_objects\"\n",
    "MODELS_PATH = PROJECT_ROOT / \"models\" / \"classification\"\n",
    "LOGS_PATH = PROJECT_ROOT / \"logs\" / \"classification\"\n",
    "\n",
    "MODELS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "LOGS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(PROJECT_ROOT / \"data\" / \"class_mapping.json\", 'r') as f:\n",
    "    class_info = json.load(f)\n",
    "NUM_CLASSES = class_info['num_classes']\n",
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20  # Start with 5 for testing\n",
    "\n",
    "print(f\"Classes: {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 697 images belonging to 9 classes.\n",
      "Found 148 images belonging to 9 classes.\n",
      "Train: 697, Val: 148\n"
     ]
    }
   ],
   "source": [
    "# Data generators with stronger augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=25,               # Increased for more face angle variety\n",
    "    width_shift_range=0.25,          # Horizontal shift\n",
    "    height_shift_range=0.25,         # Vertical shift\n",
    "    shear_range=0.2,                 # Light shear for pose variation\n",
    "    zoom_range=0.25,                 # Zoom in/out for distance changes\n",
    "    horizontal_flip=True,            # Flip horizontally (faces are symmetric)\n",
    "    brightness_range=[0.7, 1.35],    # Strong brightness variation for lighting\n",
    "    channel_shift_range=25.0,        # Color channel shift for robustness\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Validation & Test: only rescale (no augmentation)\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create generators\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    DATA_PATH / 'train',\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_gen = val_test_datagen.flow_from_directory(\n",
    "    DATA_PATH / 'val',\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Train: {train_gen.samples}, Val: {val_gen.samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Custom CNN (Simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Custom CNN...\n",
      "Params: 1,689,481\n"
     ]
    }
   ],
   "source": [
    "def build_custom_cnn():\n",
    "    \"\"\"Simplified custom CNN with better regularization\"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Block 1\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(224, 224, 3)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Block 2\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Block 3\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Block 4\n",
    "        layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        \n",
    "        # Classifier\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0005),  # Higher LR for from-scratch\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "print(\"Building Custom CNN...\")\n",
    "model_cnn = build_custom_cnn()\n",
    "print(f\"Params: {model_cnn.count_params():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING CUSTOM CNN\n",
      "================================================================================\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-31 23:35:51.494771: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.2250 - loss: 2.8928\n",
      "Epoch 1: val_accuracy improved from None to 0.10135, saving model to /Users/kevin/Documents/GitHub/Python/VESKL/11.DAE/NEU/NEU_IE7615/Prj/Discriminative/G8/Project1/IE7615_Discriminative_Project/models/classification/custom_cnn_best.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: finished saving model to /Users/kevin/Documents/GitHub/Python/VESKL/11.DAE/NEU/NEU_IE7615/Prj/Discriminative/G8/Project1/IE7615_Discriminative_Project/models/classification/custom_cnn_best.h5\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 1s/step - accuracy: 0.2783 - loss: 2.4611 - val_accuracy: 0.1014 - val_loss: 2.2658 - learning_rate: 5.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m 2/22\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 1s/step - accuracy: 0.4062 - loss: 2.1394"
     ]
    }
   ],
   "source": [
    "# Train Custom CNN\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING CUSTOM CNN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "callbacks_cnn = [\n",
    "    ModelCheckpoint(str(MODELS_PATH / 'custom_cnn_best.h5'), monitor='val_accuracy', \n",
    "                    save_best_only=True, verbose=1),\n",
    "    EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
    "]\n",
    "\n",
    "history_cnn = model_cnn.fit(\n",
    "    train_gen, epochs=EPOCHS, validation_data=val_gen,\n",
    "    callbacks=callbacks_cnn, verbose=1\n",
    ")\n",
    "\n",
    "model_cnn.save(MODELS_PATH / 'custom_cnn_last.h5')\n",
    "print(f\"\\nBest: {max(history_cnn.history['val_accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: ResNet50 (Fine-tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_resnet50():\n",
    "    \"\"\"ResNet50 with top layers unfrozen for fine-tuning\"\"\"\n",
    "    base = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    \n",
    "    # Unfreeze top 30 layers\n",
    "    base.trainable = True\n",
    "    for layer in base.layers[:-30]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    model = models.Sequential([\n",
    "        base,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0005),  # Higher for fine-tuning\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "print(\"Building ResNet50...\")\n",
    "model_resnet = build_resnet50()\n",
    "trainable = sum([tf.size(w).numpy() for w in model_resnet.trainable_weights])\n",
    "print(f\"Trainable params: {trainable:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ResNet50\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING RESNET50\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "callbacks_resnet = [\n",
    "    ModelCheckpoint(str(MODELS_PATH / 'resnet50_best.h5'), monitor='val_accuracy',\n",
    "                    save_best_only=True, verbose=1),\n",
    "    EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
    "]\n",
    "\n",
    "history_resnet = model_resnet.fit(\n",
    "    train_gen, epochs=EPOCHS, validation_data=val_gen,\n",
    "    callbacks=callbacks_resnet, verbose=1\n",
    ")\n",
    "\n",
    "model_resnet.save(MODELS_PATH / 'resnet50_last.h5')\n",
    "print(f\"\\nBest: {max(history_resnet.history['val_accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: EfficientNet-B0 (Fine-tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_efficientnet():\n",
    "    \"\"\"EfficientNet with top layers unfrozen\"\"\"\n",
    "    base = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    \n",
    "    # Unfreeze top 20 layers\n",
    "    base.trainable = True\n",
    "    for layer in base.layers[:-20]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    model = models.Sequential([\n",
    "        base,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0003),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "print(\"Building EfficientNet...\")\n",
    "model_eff = build_efficientnet()\n",
    "trainable = sum([tf.size(w).numpy() for w in model_eff.trainable_weights])\n",
    "print(f\"Trainable params: {trainable:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train EfficientNet\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING EFFICIENTNET-B0\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "callbacks_eff = [\n",
    "    ModelCheckpoint(str(MODELS_PATH / 'efficientnet_best.h5'), monitor='val_accuracy',\n",
    "                    save_best_only=True, verbose=1),\n",
    "    EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
    "]\n",
    "\n",
    "history_eff = model_eff.fit(\n",
    "    train_gen, epochs=EPOCHS, validation_data=val_gen,\n",
    "    callbacks=callbacks_eff, verbose=1\n",
    ")\n",
    "\n",
    "model_eff.save(MODELS_PATH / 'efficientnet_last.h5')\n",
    "print(f\"\\nBest: {max(history_eff.history['val_accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: MobileNetV2 (Fine-tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mobilenet():\n",
    "    \"\"\"MobileNetV2 with top layers unfrozen\"\"\"\n",
    "    base = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    \n",
    "    # Unfreeze top 20 layers\n",
    "    base.trainable = True\n",
    "    for layer in base.layers[:-20]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    model = models.Sequential([\n",
    "        base,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0003),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "print(\"Building MobileNetV2...\")\n",
    "model_mobile = build_mobilenet()\n",
    "trainable = sum([tf.size(w).numpy() for w in model_mobile.trainable_weights])\n",
    "print(f\"Trainable params: {trainable:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MobileNetV2\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING MOBILENETV2\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "callbacks_mobile = [\n",
    "    ModelCheckpoint(str(MODELS_PATH / 'mobilenet_best.h5'), monitor='val_accuracy',\n",
    "                    save_best_only=True, verbose=1),\n",
    "    EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
    "]\n",
    "\n",
    "history_mobile = model_mobile.fit(\n",
    "    train_gen, epochs=EPOCHS, validation_data=val_gen,\n",
    "    callbacks=callbacks_mobile, verbose=1\n",
    ")\n",
    "\n",
    "model_mobile.save(MODELS_PATH / 'mobilenet_last.h5')\n",
    "print(f\"\\nBest: {max(history_mobile.history['val_accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary After small Epochs Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [\n",
    "    {'model': 'Custom CNN', 'best_val_acc': max(history_cnn.history['val_accuracy'])},\n",
    "    {'model': 'ResNet50', 'best_val_acc': max(history_resnet.history['val_accuracy'])},\n",
    "    {'model': 'EfficientNet', 'best_val_acc': max(history_eff.history['val_accuracy'])},\n",
    "    {'model': 'MobileNetV2', 'best_val_acc': max(history_mobile.history['val_accuracy'])},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(results).sort_values('best_val_acc', ascending=False)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"5-EPOCH TEST RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DECISION\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nIf all models >70% after 5 epochs:\")\n",
    "print(\"  -> Increase EPOCHS to 50 and retrain all\")\n",
    "print(\"\\nIf some models <50% after 5 epochs:\")\n",
    "print(\"  -> Focus only on models with >70%\")\n",
    "print(\"  -> Adjust LR for struggling models\")\n",
    "\n",
    "best = df.iloc[0]\n",
    "print(f\"\\nCurrent best: {best['model']} at {best['best_val_acc']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After verifying small epochs work, change EPOCHS=50 and rerun"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
